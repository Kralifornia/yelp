{
 "metadata": {
  "name": "",
  "signature": "sha256:10073aacfe848b8d3fed71c62f729fd76f952f6b41b3dcb8d6c58cbd4629ce5d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Tip Sentiment Analysis From The Yelp Phoenix Data Set\n",
      "In this notebook we are going to analyze the tips the users have written after visiting a business and obtain the polarity of their opinions regarding the different products or items each business have to offer.\n",
      "\n",
      "We will measure the polarity of the opinions about items by taking each sentence in the tip and tagging every word to identify whether its a nound, adjective, verb, adverb, etc. Once we have identified each word, for each noun in the sentence, we will take every adjective in the sentence, determine its polarity and calculate the polarity given to the noun in that sentence. In the following sections we are going to explain the details of how is this done.\n",
      "\n",
      "## Sentence tokenizer\n",
      "The first step into this implementation of sentiment analysis of items is to take every tip and split it into sentences. Since each sentence expresses an idea, we are going to analyze each sentence separately. In python we can do this easily by using a tokenizer, for example:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "my_text = \"The burgers are very good. The service is bad. They open untill very late and the drinks are quite expensive.\"\n",
      "print('\\n-----\\n'.join(sentence_tokenizer.tokenize(my_text.strip())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The burgers are very good.\n",
        "-----\n",
        "The service is bad.\n",
        "-----\n",
        "They open untill very late and the drinks are quite expensive.\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part of speech tagger\n",
      "Now that we have separated each sentence, we can start to do part of speech tagging.\n",
      "To do this, we have to split each sentence into words and them tag those words. Thanks to NLTK this is also very straigthforward:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Example\n",
      "my_sentence = sentence_tokenizer.tokenize(my_text.strip())[0]\n",
      "my_words = nltk.word_tokenize(my_sentence)\n",
      "my_tagged_words = nltk.pos_tag(my_words)\n",
      "print(my_tagged_words)\n",
      "\n",
      "#Function\n",
      "def tag_text(text):\n",
      "    words = nltk.word_tokenize(text)\n",
      "    tagged_words = nltk.pos_tag(words)\n",
      "    return tagged_words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('The', 'DT'), ('burgers', 'NNS'), ('are', 'VBP'), ('very', 'RB'), ('good', 'JJ'), ('.', '.')]\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you can see in the example above, the words are now tagged in a pair (word, tag). The meaning is the tags above is the following:\n",
      "\n",
      "* DT: Determiner.\n",
      "* NND: Plural noun.\n",
      "* VBP: Singular verb in present tense and non third person.\n",
      "* RB: Adverb.\n",
      "* JJ: Adjective.\n",
      "\n",
      "There are much more tags, but for our analysis we just care for the nouns and the adjective. All the tags for nouns start with 'NN' and all the tags for adjectives start with 'JJ'.\n",
      "\n",
      "## Words polarity\n",
      "The next step is to determine whether the adjectives that come along with the nouns are positive or negative. This will help us to determine how positive or negative is the review about the item (noun).\n",
      "\n",
      "We can do this manually giving every word a score between [-1, 1]. For instance, awesome will be graded with 0.8, lousy -0.6, and so on.\n",
      "\n",
      "Nevertheless, there is a database that contains the polarity of thousands of words in the english language. We can use this database instead and save us the work of grading all the words manually. The database can be found at http://sentiwordnet.isti.cnr.it/\n",
      "\n",
      "Since the database is in a CSV file, we are going to have to parse it and store it in a friendly format we can use easily in python. In this case we will use a dictionary. The following function will enable us to do that."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Created by\n",
      "def load_sent_word_net():\n",
      "\n",
      "    sent_scores = collections.defaultdict(list)\n",
      "\n",
      "    with open(\"data/SentiWordNet_3.0.0_20130122.txt\", \"r\") as csvfile:\n",
      "        reader = csv.reader(csvfile, delimiter='\\t', quotechar='\"')\n",
      "        for line in reader:\n",
      "            if line[0].startswith(\"#\"):\n",
      "                continue\n",
      "            if len(line) == 1:\n",
      "                continue\n",
      "\n",
      "            POS, ID, PosScore, NegScore, SynsetTerms, Gloss = line\n",
      "            if len(POS) == 0 or len(ID) == 0:\n",
      "                continue\n",
      "            # print POS,PosScore,NegScore,SynsetTerms\n",
      "            for term in SynsetTerms.split(\" \"):\n",
      "                # drop #number at the end of every term\n",
      "                term = term.split(\"#\")[0]\n",
      "                term = term.replace(\"-\", \" \").replace(\"_\", \" \")\n",
      "                key = \"%s/%s\" % (POS, term.split(\"#\")[0])\n",
      "                sent_scores[key].append((float(PosScore), float(NegScore)))\n",
      "    for key, value in sent_scores.iteritems():\n",
      "        sent_scores[key] = numpy.mean(value, axis=0)\n",
      "\n",
      "    return sent_scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can easily calculate the polarity of a word, to do that, we have defined the following function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def calculate_word_score(tagged_word):\n",
      "\n",
      "    word = tagged_word[0].lower()\n",
      "    tag = tagged_word[1]\n",
      "\n",
      "    if tag.startswith('JJ'):\n",
      "        tag = 'a'\n",
      "    else:\n",
      "        return 0\n",
      "\n",
      "    dict_word = tag + '/' + word\n",
      "    total_score = 0\n",
      "\n",
      "    if dict_word in sentiment_words:\n",
      "        score = sentiment_words[dict_word]\n",
      "        total_score = score[0] - score[1]\n",
      "\n",
      "    return total_score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Sentiment analysis\n",
      "With all the above, we now have enough tools to determine the polarity of what its being said about an item.\n",
      "We are going to create a function which returns a numerical value that refers on how good are the adjectives around a noun in a sentence. This function first obtains the polarity of the adjective and then calculates the distance from that adjective to the noun. The formula is:\n",
      "\n",
      "$$\n",
      "\\sum_{a \\in S} \\frac{polarity(a)}{distance_{a-n}}\n",
      "$$\n",
      "\n",
      "Where:\n",
      "\n",
      "* S: is a sentence.\n",
      "* a: refers to each adjective in the sentence S.\n",
      "* polarity: calculates the polarity of the adjective a.\n",
      "* distance_{a-n}: is the distance that there is from the adjective a to the noun n.\n",
      "\n",
      "The code for the function that calculates how good is the polarity for a noun is:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stemmer = nltk.stem.SnowballStemmer('english')\n",
      "\n",
      "def calculate_weight(index1, index2):\n",
      "    if index1 == index2:\n",
      "        return 0\n",
      "    return 1.0 / math.fabs(index1 - index2)\n",
      "\n",
      "def grade_noun(noun, noun_index, tagged_words):\n",
      "    for adjective_index, (adjective, adjective_tag) in enumerate(tagged_words):\n",
      "        if adjective_tag.startswith(\"JJ\"):\n",
      "            weight = calculate_weight(noun_index, adjective_index)\n",
      "            score = calculate_word_score((adjective, adjective_tag))\n",
      "            noun = stemmer.stem(noun)\n",
      "            if noun in noun_dictionary:\n",
      "                noun_dictionary[noun] += score * weight\n",
      "            else:\n",
      "                noun_dictionary[noun] = score * weight"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once we have graded the noun, we will store its score in a dictionary which will be uptaded everytime that we find the noun in another sentce. Since many words can refer to the same, we will use a stemmer to avoid splitting grades for words that mean the same (e.g, the words 'burger' and 'burgers')\n",
      "\n",
      "The above code is good for just one noun, but since we have to do the same for all the nouns in a sentence, we will have to introduce a loop:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def grade_sentence(sentence):\n",
      "    tagged_words = tag_text(sentence)\n",
      "\n",
      "    for noun_index, (noun, noun_tag) in enumerate(tagged_words):\n",
      "        if noun_tag.startswith(\"NN\"):\n",
      "            grade_noun(noun, noun_index, tagged_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we do the same for every tip in the database:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def grade_sentences(sentences):\n",
      "    for sentence in sentences:\n",
      "        grade_sentence(sentence)\n",
      "\n",
      "def analyze_tips(tips):\n",
      "    for tip in tips:\n",
      "        sentences = sentence_tokenizer.tokenize(tip)\n",
      "        grade_sentences(sentences)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Putting all together\n",
      "Finally we can do some sentiment analysis in the tips database. We are going to analyze one business at a time, to see how good or bad are the items in a business based on the users'reviews.\n",
      "\n",
      "But before that, we have to define some auxiliary functions:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "def load_json_file(file_path):\n",
      "    \"\"\"\n",
      "    Builds a list of dictionaries from a JSON file\n",
      "\n",
      "    :type file_path: string\n",
      "    :param file_path: the path for the file that contains the businesses\n",
      "    data\n",
      "    :return: a list of dictionaries with the data from the files\n",
      "    \"\"\"\n",
      "    records = [json.loads(line) for line in open(file_path)]\n",
      "\n",
      "    return records\n",
      "\n",
      "def filter_records(records, field, values):\n",
      "    filtered_records = [record for record in records if\n",
      "                        record[field] in values]\n",
      "    return filtered_records"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import collections\n",
      "import csv\n",
      "\n",
      "tip_file_path = 'data/yelp_academic_dataset_tip.json'\n",
      "my_records = load_json_file(tip_file_path)\n",
      "sentiment_words = load_sent_word_net()\n",
      "noun_dictionary = {}\n",
      "\n",
      "def analyze_business(business_id):\n",
      "    noun_dictionary = {}\n",
      "    business_records = filter_records(my_records, 'business_id', [business_id])\n",
      "    my_tips = [my_record['text'] for my_record in business_records]\n",
      "    analyze_tips(my_tips)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Results\n",
      "\n",
      "### Airport example\n",
      "For our first analysis, we are going to take the tips of an airport and see what people are talking about"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import operator\n",
      "analyze_business('hW0Ne_HTHEAgGF1rAdmR-g')\n",
      "sorted_x = sorted(noun_dictionary.iteritems(), key=operator.itemgetter(1))\n",
      "print(\"Worst items:\")\n",
      "print(sorted_x[:10])\n",
      "print(\"Best items:\")\n",
      "print(sorted_x[-10:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you can see in the above results, people complain about security controls, wifi and long lines. This is very typical in most of the airports around the world. On the positive side, people talk well about the airport and the food.\n",
      "\n",
      "### Restaurant example\n",
      "We are going to take two restaurants and see what people are saying about their items:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "noun_dictionary = {}\n",
      "print(\"Restaurant 1:\")\n",
      "analyze_business('-sC66z4SO3tR7nFCjfQwuQ')\n",
      "sorted_x = sorted(noun_dictionary.iteritems(), key=operator.itemgetter(1))\n",
      "print(\"Worst items:\")\n",
      "print(sorted_x[:10])\n",
      "print(\"Best items:\")\n",
      "print(sorted_x[-10:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the first restaurant, people seem to have complained about the bartenders and the quesadillas, it seems also that coffe and lemonade are not that good. On the otger side, if you go to this place, you should definetly try the tacos, the bloody maries and the margarita. It seems that the place has very good food and service."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "noun_dictionary = {}\n",
      "print(\"Restaurant 2:\")\n",
      "analyze_business('WS1z1OAR0tRl4FsjdTGUFQ')\n",
      "sorted_x = sorted(noun_dictionary.iteritems(), key=operator.itemgetter(1))\n",
      "print(\"Worst items:\")\n",
      "print(sorted_x[:10])\n",
      "print(\"Best items:\")\n",
      "print(sorted_x[-10:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the second restaurant, people have complained about the gluten, the parking spaces and the beans. It seems like a very good place to go on sundays to have a brunch and where you will have a friendly service."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}