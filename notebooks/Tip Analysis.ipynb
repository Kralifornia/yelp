{
 "metadata": {
  "name": "",
  "signature": "sha256:da3596edca6ea8da522fe79a2f01133ad131a55881a552b9f7288a3396b90ef5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "from pandas import DataFrame\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "import numpy as np\n",
      "import nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_file(file_path):\n",
      "    records = [json.loads(line) for line in open(file_path)]\n",
      "\n",
      "    return records"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "def drop_fields(fields, dictionary_list):\n",
      "    \"\"\"\n",
      "    Removes the specified fields from every dictionary in the list records\n",
      "\n",
      "    :rtype : void\n",
      "    :param fields: a list of strings, which contains the fields that are\n",
      "    going to be removed from every dictionary in the list records\n",
      "    :param dictionary_list: a list of dictionaries\n",
      "    \"\"\"\n",
      "    for record in dictionary_list:\n",
      "        for field in fields:\n",
      "            del (record[field])\n",
      "\n",
      "\n",
      "def filter_records(records, field, values):\n",
      "    filtered_records = [record for record in records if\n",
      "                        record[field] in values]\n",
      "    return filtered_records"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We analyze the tip database and select the businesses that have the higher number of reviews"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def analyze(file_path, n):\n",
      "        records = load_file(file_path)\n",
      "        drop_fields(['text', 'type', 'date', 'user_id', 'likes'],\n",
      "                             records)\n",
      "        data_frame = DataFrame(records)\n",
      "        counts = data_frame.groupby('business_id').size()\n",
      "        counts.sort(ascending=0)\n",
      "        top_counts = counts[:n]\n",
      "        print(top_counts)\n",
      "\n",
      "        print records[0].keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we will take the top ten businesses with more reviews"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_folder = 'data/'\n",
      "tip_file_path = data_folder + 'yelp_academic_dataset_tip.json'\n",
      "analyze(tip_file_path, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "business_id\n",
        "hW0Ne_HTHEAgGF1rAdmR-g    1420\n",
        "JokKtdXU7zXHcr20Lrk29A     391\n",
        "0UZ31UTcOLRKuqPqPe-VBA     355\n",
        "aRkYtXfmEKYG-eTDf_qUsw     336\n",
        "-sC66z4SO3tR7nFCjfQwuQ     291\n",
        "_FXql6eVhbM923RdCi94SA     284\n",
        "EWMwV5V9BxNs_U6nNVMeqw     248\n",
        "L9UYbtAUOcfTgZFimehlXw     243\n",
        "uFJwKlHL6HyHSJmORO8-5w     242\n",
        "WS1z1OAR0tRl4FsjdTGUFQ     239\n",
        "dtype: int64\n",
        "[u'business_id']\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk.stem\n",
      "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "\n",
      "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
      "\n",
      "    def build_analyzer(self):\n",
      "        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n",
      "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we create a function to apply TF-IDF to the text inside the tips"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tf_idf(file_path, business_id, stem):\n",
      "        records = load_file(file_path)\n",
      "        data = [record['text'] for record in records]\n",
      "        vectorizer = TfidfVectorizer(min_df=1, stop_words='english')\n",
      "        if stem:\n",
      "            vectorizer = StemmedTfidfVectorizer(min_df=1, stop_words='english')\n",
      "        train = vectorizer.fit_transform(data)\n",
      "        #print \"Vocabulary:\", vectorizer.get_feature_names()\n",
      "        num_samples, num_features = train.shape\n",
      "        print(\"#samples: %d, #features: %d\" % (\n",
      "            num_samples, num_features))\n",
      "\n",
      "        business_records = filter_records(records, 'business_id', [business_id])\n",
      "        business_data = [record['text'] for record in business_records]\n",
      "        freq_term_matrix = vectorizer.transform(business_data)\n",
      "        vocabulary = vectorizer.get_feature_names()\n",
      "\n",
      "        my_list = []\n",
      "        rows, cols = freq_term_matrix.nonzero()\n",
      "        for row, col in zip(rows, cols):\n",
      "            my_dict = {}\n",
      "            word = vocabulary[col]\n",
      "            my_dict['tip_id'] = row\n",
      "            my_dict['word'] = word\n",
      "            my_dict['tfidf'] = freq_term_matrix[row, col]\n",
      "            my_list.append(my_dict)\n",
      "\n",
      "        data_frame = DataFrame(my_list)\n",
      "        suma = data_frame.groupby('word').aggregate(np.sum)['tfidf']\n",
      "        ordenado = suma.order()\n",
      "        print ordenado"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We execute the code to see the most relevant words in the tips"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tf_idf(tip_file_path, 'EWMwV5V9BxNs_U6nNVMeqw', 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#samples: 113993, #features: 32066\n",
        "word\n",
        "hot         0.198456\n",
        "make        0.202844\n",
        "az          0.203616\n",
        "door        0.220653\n",
        "spinach     0.224708\n",
        "gone        0.229656\n",
        "taste       0.230293\n",
        "lots        0.231347\n",
        "said        0.231943\n",
        "coming      0.234097\n",
        "pie         0.239452\n",
        "does        0.244336\n",
        "add         0.248715\n",
        "bathroom    0.250183\n",
        "ambience    0.250342\n",
        "...\n",
        "night       3.509764\n",
        "hummus      3.674564\n",
        "sweet       3.981846\n",
        "service     4.056771\n",
        "potato      4.193176\n",
        "food        4.232536\n",
        "place       4.393503\n",
        "best        5.365621\n",
        "burgers     5.517702\n",
        "love        5.891557\n",
        "great       7.127851\n",
        "fries       7.444558\n",
        "burger      7.939506\n",
        "brunch      8.035746\n",
        "fez        17.164686\n",
        "Name: tfidf, Length: 611, dtype: float64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we execute the same code but stemming the words, so that we don't have words with the same meaning like burger and burgers repeated"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tf_idf(tip_file_path, 'EWMwV5V9BxNs_U6nNVMeqw', 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#samples: 113993, #features: 24571\n",
        "word\n",
        "make       0.191378\n",
        "hot        0.201586\n",
        "custom     0.212081\n",
        "az         0.216548\n",
        "tast       0.219641\n",
        "door       0.228683\n",
        "spinach    0.233878\n",
        "pass       0.235770\n",
        "gone       0.239028\n",
        "said       0.239881\n",
        "pie        0.243106\n",
        "cake       0.248671\n",
        "order      0.249398\n",
        "doe        0.252706\n",
        "add        0.254718\n",
        "...\n",
        "servic       4.173222\n",
        "sweet        4.175118\n",
        "potato       4.201844\n",
        "food         4.341459\n",
        "martini      4.371138\n",
        "special      4.523931\n",
        "place        4.668387\n",
        "cocktail     4.746489\n",
        "best         5.580851\n",
        "fri          7.059537\n",
        "love         7.082217\n",
        "great        7.569483\n",
        "brunch       8.266688\n",
        "burger      12.550479\n",
        "fez         17.762831\n",
        "Name: tfidf, Length: 562, dtype: float64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    }
   ],
   "metadata": {}
  }
 ]
}