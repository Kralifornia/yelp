{
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Business & Tip Analysis from the Yelp Phoenix Data Set Using Clustering\n",
      "We are going to analyze two data sets that are related from the Yelp Phoenix data set in order to find patterns that can reveal us valuable informantion. Firstly we will deal with the data from the tips data set\n",
      "\n",
      "## Tip Analysis\n",
      "For each record in the data set we are going to extract the text of the tip and then process it using TF-IDF, finally we are going to perform some clustering on the resulting data.\n",
      "\n",
      "### ETL\n",
      "To extract the data, we just have to parse the JSON file, the JSON package will help us to achieve this, its pretty straightforward."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "import json\n",
      "\n",
      "def load_json_file(file_path):\n",
      "    \"\"\"\n",
      "    Builds a list of dictionaries from a JSON file\n",
      "\n",
      "    :type file_path: string\n",
      "    :param file_path: the path for the file that contains the businesses\n",
      "    data\n",
      "    :return: a list of dictionaries with the data from the files\n",
      "    \"\"\"\n",
      "    records = [json.loads(line) for line in open(file_path)]\n",
      "\n",
      "    return records"
     ],
     "language": "python",
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### TF-IDF\n",
      "To apply TF-IDF on the data we are going to use the sklearn package, specifically the class TfidfVectorizer, we will use this class along a Stemmer which will help us group words that have the same root, for instance the words tall, taller and tallest will all be grouped into tall."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "import nltk.stem\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
      "\n",
      "\n",
      "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
      "\n",
      "    def build_analyzer(self):\n",
      "        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n",
      "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
     ],
     "language": "python",
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once we have defined this class, we can go on and create a matrix which will contain the term frequency-inverse document frequency of each tip."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "def tf_idf_tips(file_path):\n",
      "    records = load_json_file(file_path)\n",
      "    data = [record['text'] for record in records]\n",
      "    vectorizer = StemmedTfidfVectorizer(min_df=1, stop_words='english')\n",
      "    vectorized = vectorizer.fit_transform(data)\n",
      "    num_samples, num_features = vectorized.shape\n",
      "    print(\"#samples: %d, #features: %d\" % (\n",
      "        num_samples, num_features))\n",
      "\n",
      "    return vectorized"
     ],
     "language": "python",
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Clustering\n",
      "The matrix we have created in the previous step serves as an input for a clustering algorithm. If we cluster the data we can use it to obtain tips that are similar to a given tip, which can be useful for a search engine.\n",
      "\n",
      "We are going to define a function that will help us to cluster, and other function to evaluate how good the clusters are using Silhouette score"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "import nltk\n",
      "import numpy\n",
      "import sklearn.cluster as skcluster\n",
      "import sklearn.metrics as skmetrics\n",
      "import time\n",
      "import scipy.cluster as scicluster\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def k_means_scikit(matrix):\n",
      "    k_means = skcluster.KMeans(n_clusters=50, init='k-means++',\n",
      "                               n_init=1, verbose=0)\n",
      "    k_means.fit(matrix)\n",
      "\n",
      "    return k_means.labels_\n",
      "\n",
      "def evaluate_performance(data, labels, metric='euclidean'):\n",
      "    score = skmetrics.silhouette_score(data, labels, metric=metric)\n",
      "    print('Score:', score)\n",
      "\n",
      "    return score\n",
      "\n",
      "def cluster_and_evaluate_data(matrix, metric='euclidean'):\n",
      "    print('Clustering started')\n",
      "    clustering_start = time.time()\n",
      "    labels = k_means_scikit(matrix)\n",
      "    clustering_total = time.time() - clustering_start\n",
      "    print('Clustering time:', clustering_total)\n",
      "\n",
      "    evaluation_start = time.time()\n",
      "    score = evaluate_performance(matrix, labels, metric)\n",
      "    evaluation_total = time.time() - evaluation_start\n",
      "    print('Evaluation time:', evaluation_total)\n",
      "    total = time.time() - clustering_start\n",
      "    print('Total time:', total)"
     ],
     "language": "python",
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given the above functions we can now perform clustering and evaluate how good the clusters are using Silhouette score. The closer the value of the score to 1 the better, if its close to -1 is very bad, an if its close to 0 it means that the clusters are overlapping.\n",
      "\n",
      "We are going to use just 100 samples because we are executing this example in a navigator, be sure to remove the [:100] when executing this in your machine. For the whole data set, the k-means algorithm was executed successfully but the silhouette score ran out of memory."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#samples: 113993, #features: 24571\n",
        "Clustering started"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('Clustering time:', 0.20326685905456543)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('Score:', 0.011462435659864297)\n",
        "('Evaluation time:', 0.04374504089355469)\n",
        "('Total time:', 0.24753999710083008)\n"
       ]
      }
     ],
     "input": [
      "tip_file_path = 'data/yelp_academic_dataset_tip.json'\n",
      "tip_matrix = tf_idf_tips(tip_file_path)\n",
      "cluster_and_evaluate_data(tip_matrix[:100])"
     ],
     "language": "python",
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Business Analysis\n",
      "For each record in the data set we are going to extract the categories associated to that particular business, we will transform all of the data into a binary matrix of N rows and M columns, where N is the number of businesses and M the number of categories. If the business <i>i</i> is in the category <i>j</i> then the cell at <i>i,j</i> will have a 1, otherwise it will have a 0.\n",
      "\n",
      "We will also create a list of sets that contains the categories of each business. This set will be very useful when clustering using Jaccard similarity.\n",
      "\n",
      "### ETL\n",
      "First we are going to create the binary matrix, in order to do that we will have to define several auxiliary functions first\n"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "def drop_fields(fields, dictionary_list):\n",
      "    \"\"\"\n",
      "    Removes the specified fields from every dictionary in the list records\n",
      "\n",
      "    :rtype : void\n",
      "    :param fields: a list of strings, which contains the fields that are\n",
      "    going to be removed from every dictionary in the list records\n",
      "    :param dictionary_list: a list of dictionaries\n",
      "    \"\"\"\n",
      "    for record in dictionary_list:\n",
      "        for field in fields:\n",
      "            del (record[field])\n",
      "\n",
      "def filter_records(records, field, values):\n",
      "    filtered_records = [record for record in records if\n",
      "                        record[field] in values]\n",
      "    return filtered_records\n",
      "\n",
      "def add_transpose_list_column(field, dictionary_list):\n",
      "    \"\"\"\n",
      "    Takes a list of dictionaries and adds to every dictionary a new field\n",
      "    for each value contained in the specified field among all the\n",
      "    dictionaries in the field, leaving 1 for the values that are present in\n",
      "    the dictionary and 0 for the values that are not. It can be seen as\n",
      "    transposing the dictionary matrix.\n",
      "\n",
      "    :param field: the field which is going to be transposed\n",
      "    :param dictionary_list: a list of dictionaries\n",
      "    :return: the modified list of dictionaries\n",
      "    \"\"\"\n",
      "    values_set = set()\n",
      "    for dictionary in dictionary_list:\n",
      "        values_set |= set(dictionary[field])\n",
      "\n",
      "    for dictionary in dictionary_list:\n",
      "        for value in values_set:\n",
      "            if value in dictionary[field]:\n",
      "                dictionary[value] = 1\n",
      "            else:\n",
      "                dictionary[value] = 0\n",
      "\n",
      "    return dictionary_list\n",
      "\n",
      "def add_transpose_single_column(field, dictionary_list):\n",
      "    \"\"\"\n",
      "    Takes a list of dictionaries and adds to every dictionary a new field\n",
      "    for each value contained in the specified field among all the\n",
      "    dictionaries in the field, leaving 1 for the values that are present in\n",
      "    the dictionary and 0 for the values that are not. It can be seen as\n",
      "    transposing the dictionary matrix.\n",
      "\n",
      "    :param field: the field which is going to be transposed\n",
      "    :param dictionary_list: a list of dictionaries\n",
      "    :return: the modified list of dictionaries\n",
      "    \"\"\"\n",
      "\n",
      "    values_set = set()\n",
      "    for dictionary in dictionary_list:\n",
      "        values_set.add(dictionary[field])\n",
      "\n",
      "    for dictionary in dictionary_list:\n",
      "        for value in values_set:\n",
      "            if value in dictionary[field]:\n",
      "                dictionary[value] = 1\n",
      "            else:\n",
      "                dictionary[value] = 0\n",
      "\n",
      "    return dictionary_list\n",
      "\n",
      "def drop_unwanted_fields(dictionary_list):\n",
      "    \"\"\"\n",
      "    Drops fields that are not useful for data analysis in the business\n",
      "    data set\n",
      "\n",
      "    :rtype : void\n",
      "    :param dictionary_list: the list of dictionaries containing the data\n",
      "    \"\"\"\n",
      "    unwanted_fields = [\n",
      "        'attributes',\n",
      "        'business_id',\n",
      "        'categories',\n",
      "        'city',\n",
      "        'full_address',\n",
      "        'latitude',\n",
      "        'longitude',\n",
      "        'hours',\n",
      "        'name',\n",
      "        'neighborhoods',\n",
      "        'open',\n",
      "        'review_count',\n",
      "        'stars',\n",
      "        'state',\n",
      "        'type'\n",
      "    ]\n",
      "\n",
      "    drop_fields(unwanted_fields, dictionary_list)"
     ],
     "language": "python",
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can define our function to create the matrix"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "def create_category_matrix(file_path):\n",
      "    \"\"\"\n",
      "    Creates a matrix with all the categories for businesses that are\n",
      "    contained in the Yelp Phoenix Business data set. Each column of the\n",
      "    matrix represents a category, and each row a business. This is a binary\n",
      "    matrix that contains a 1 at the position i,j if the business i contains\n",
      "    the category j, and a 0 otherwise.\n",
      "\n",
      "    :rtype : numpy array matrix\n",
      "    :param file_path: the path for the file that contains the businesses\n",
      "    data\n",
      "    :return: a numpy array binary matrix\n",
      "    \"\"\"\n",
      "    records = load_json_file(file_path)\n",
      "\n",
      "    # Now we obtain the categories for all the businesses\n",
      "    records = add_transpose_list_column('categories', records)\n",
      "    drop_unwanted_fields(records)\n",
      "    matrix = numpy.array(\n",
      "        [numpy.array(record.values()) for record in records])\n",
      "\n",
      "    return matrix"
     ],
     "language": "python",
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will also define another function to create a list of sets containing the categories for each business"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "def create_category_sets(file_path):\n",
      "    \"\"\"\n",
      "    Creates an array of arrays in which each sub-array contains the\n",
      "    categories of each business in the Yelp Phoenix Business data set\n",
      "\n",
      "    :rtype : numpy array matrix\n",
      "    :param file_path: the path for the file that contains the businesses\n",
      "    data\n",
      "    :return: a numpy array of numpy arrays with the categories that each\n",
      "    business has, for example [['Restaurant', 'Mexican', 'Bar'],\n",
      "    ['Bar', 'Disco']]\n",
      "    \"\"\"\n",
      "    records = load_json_file(file_path)\n",
      "    sets = numpy.array(\n",
      "        [set(record['categories']) for record in records])\n",
      "\n",
      "    return sets"
     ],
     "language": "python",
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Clustering\n",
      "Now that we have a function that creates the binary matrix that serves as an input to the clustering function, we can go ahead and perform some clustering"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Clustering started\n",
        "('Clustering time:', 0.15648913383483887)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('Score:', 0.01920154263570531)\n",
        "('Evaluation time:', 0.04501914978027344)\n",
        "('Total time:', 0.2019340991973877)\n"
       ]
      }
     ],
     "input": [
      "business_file_path = 'data/yelp_academic_dataset_business.json'\n",
      "categories_matrix = create_category_matrix(business_file_path)\n",
      "cluster_and_evaluate_data(tip_matrix[:100])"
     ],
     "language": "python",
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Other Clustering Techniques\n",
      "Among the packages nltk, sklearn and scipy.cluster, there are several other techniques than can be used for clustering. We have gathered some of them in the above class. The fastest algorithm is k-means."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "import nltk\n",
      "import numpy\n",
      "import sklearn.cluster as skcluster\n",
      "import sklearn.metrics as skmetrics\n",
      "import time\n",
      "import scipy.cluster as scicluster\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.spatial.distance as distance\n",
      "\n",
      "__author__ = 'franpena'\n",
      "\n",
      "\n",
      "class Clusterer:\n",
      "    def __init__(self):\n",
      "        pass\n",
      "\n",
      "    # OK\n",
      "    @staticmethod\n",
      "    def k_means_scikit(matrix):\n",
      "        k_means = skcluster.KMeans(n_clusters=50, init='k-means++',\n",
      "                                   n_init=1, verbose=1)\n",
      "        k_means.fit(matrix)\n",
      "\n",
      "        return k_means.labels_\n",
      "\n",
      "    # OK\n",
      "    # With cosine distance the algorithm doesn't converge\n",
      "    @staticmethod\n",
      "    def k_means_nltk(matrix):\n",
      "        clusterer = nltk.KMeansClusterer(50, nltk.euclidean_distance,\n",
      "                                         avoid_empty_clusters=True,\n",
      "                                         conv_test=10)\n",
      "        labels = numpy.array(clusterer.cluster(matrix, True, trace=True))\n",
      "\n",
      "        return labels\n",
      "\n",
      "    # OK\n",
      "    @staticmethod\n",
      "    def gaac(matrix):\n",
      "        clusterer = nltk.GAAClusterer()\n",
      "        labels = numpy.array(clusterer.cluster(matrix, False, trace=True))\n",
      "        dendrogram = clusterer.dendrogram()\n",
      "        # dendrogram.show()\n",
      "\n",
      "        return labels\n",
      "\n",
      "    # OK\n",
      "    @staticmethod\n",
      "    def k_means_scipy(matrix):\n",
      "        centroids, distortion = scicluster.vq.kmeans(matrix, 50, thresh=0.1)\n",
      "\n",
      "        print('Centroids:', centroids)\n",
      "        print('Distortion:', distortion)\n",
      "\n",
      "    @staticmethod\n",
      "    def linkage(matrix):\n",
      "        linkage_matrix = scicluster.hierarchy.linkage(matrix)\n",
      "\n",
      "        print('Linkage matrix:', linkage_matrix)\n",
      "\n",
      "        dendrogram = scicluster.hierarchy.dendrogram(linkage_matrix)\n",
      "        ax = plt.gca()\n",
      "        xlbls = ax.get_xmajorticklabels()\n",
      "        plt.show()\n",
      "\n",
      "        leaves = dendrogram['leaves']\n",
      "        print(leaves)\n",
      "\n",
      "    @staticmethod\n",
      "    def mean_shift(matrix):\n",
      "        mean_shift = skcluster.MeanShift()\n",
      "        mean_shift.fit(matrix)\n",
      "\n",
      "        labels = mean_shift.labels_\n",
      "        # Number of clusters in labels, ignoring noise if present.\n",
      "        n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
      "        print('Estimated number of clusters:', n_clusters_)\n",
      "\n",
      "        return labels\n",
      "\n",
      "    @staticmethod\n",
      "    def ward(matrix):\n",
      "        ward = skcluster.Ward(n_clusters=50, compute_full_tree=False)\n",
      "        ward.fit(matrix)\n",
      "\n",
      "        return ward.labels_\n",
      "\n",
      "    @staticmethod\n",
      "    def dbscan(matrix):\n",
      "        dbscan = skcluster.DBSCAN(eps=0.3, min_samples=50, metric='euclidean')\n",
      "        # dbscan = skcluster.DBSCAN(eps=0.3, min_samples=50,\n",
      "        #                           metric=nltk.cosine_distance)\n",
      "        dbscan.fit(matrix)\n",
      "\n",
      "        labels = dbscan.labels_\n",
      "        # Number of clusters in labels, ignoring noise if present.\n",
      "        n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
      "        print('Estimated number of clusters:', n_clusters_)\n",
      "\n",
      "        return labels\n",
      "\n",
      "    # OK\n",
      "    @staticmethod\n",
      "    def evaluate_performance(data, labels, metric='euclidean'):\n",
      "        score = skmetrics.silhouette_score(data, labels, metric=metric)\n",
      "        print('Labels:', labels)\n",
      "        print('Score:', score)\n",
      "\n",
      "        return score\n",
      "\n",
      "    @staticmethod\n",
      "    def cluster_data(matrix, algorithm):\n",
      "\n",
      "        if algorithm == 'k-means-scikit':\n",
      "            return Clusterer.k_means_scikit(matrix)\n",
      "        elif algorithm == 'k-means-nltk':\n",
      "            return Clusterer.k_means_nltk(matrix)\n",
      "        # elif algorithm == 'k-means-scipy':\n",
      "        #     return BusinessClusterer.k_means_scipy(matrix)\n",
      "        elif algorithm == 'mean-shift':\n",
      "            return Clusterer.mean_shift(matrix)\n",
      "        elif algorithm == 'ward':\n",
      "            return Clusterer.ward(matrix)\n",
      "        elif algorithm == 'dbscan':\n",
      "            return Clusterer.dbscan(matrix)\n",
      "\n",
      "    @staticmethod\n",
      "    def cluster_and_evaluate_data(matrix, algorithm, metric='euclidean'):\n",
      "        print('Clustering started using ' + algorithm)\n",
      "        clustering_start = time.time()\n",
      "        labels = Clusterer.cluster_data(matrix, algorithm)\n",
      "        clustering_total = time.time() - clustering_start\n",
      "        print('Clustering time:', clustering_total)\n",
      "\n",
      "        evaluation_start = time.time()\n",
      "        score = Clusterer.evaluate_performance(matrix, labels, metric)\n",
      "        evaluation_total = time.time() - evaluation_start\n",
      "        print('Evaluation time:', evaluation_total)\n",
      "        total = time.time() - clustering_start\n",
      "        print('Total time:', total)"
     ],
     "language": "python",
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following are some results after performing clustering on the binary categories matrix:\n",
      "\n",
      "Clustering started using dbscan\n",
      "min_samples=10\n",
      "('Estimated number of clusters:', 474)\n",
      "('Clustering time:', 269.36252093315125)\n",
      "('Score:', 0.69331718210204796)\n",
      "('Evaluation time:', 639.6426520347595)\n",
      "('Total time:', 909.005243062973)\n",
      "\n",
      "Clustering started using dbscan\n",
      "min_samples=50\n",
      "('Estimated number of clusters:', 48)\n",
      "('Clustering time:', 236.92400288581848)\n",
      "('Score:', 0.15291211349926839)\n",
      "('Evaluation time:', 124.55789494514465)\n",
      "('Total time:', 361.48195600509644)\n",
      "\n",
      "Clustering started using k-means-scikit\n",
      "('Clustering time:', 11.542982816696167)\n",
      "('Score:', 0.28384880845352284)\n",
      "('Evaluation time:', 107.42438888549805)\n",
      "('Total time:', 118.9674379825592)\n",
      "\n",
      "Clustering started using k-means-nltk\n",
      "('Clustering time:', 12.875365018844604)\n",
      "('Labels:', array([42, 24, 32, ..., 10, 45,  7]))\n",
      "('Score:', 0.24563920653172072)\n",
      "('Evaluation time:', 120.07143902778625)\n",
      "('Total time:', 132.9468560218811)\n",
      "\n",
      "Also to note:\n",
      "\n",
      "* k-means using cosine distance does not converge\n",
      "* mean shift algorithm didn't converged\n",
      "* There are some clusters algorithms for hierarchy clustering such as ward, linkage and gcaa (uses cosine distance), but the number of clusters they generate is very big and therefore the results are hard to interpret\n",
      "* Using dbscan with cosine produced just one cluster\n",
      "* None of the algorithms worked using jaccard similarity\n",
      "\n",
      "In order to have clearer and more interpretable results, a testing framework should be developed, which presents the scores of the clustering algorithms and the values for each parameter."
     ]
    }
   ]
  }
 ],
 "cells": [],
 "metadata": {
  "name": "",
  "signature": "sha256:7afddf04fc18369b6f61850ed71a1838f22efe5a7f2a7d3621ad4c46ed08c65e"
 },
 "nbformat": 3,
 "nbformat_minor": 0
}